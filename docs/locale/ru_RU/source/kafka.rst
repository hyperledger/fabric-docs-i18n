Создание службы упорядочения на основе Kafka
============================================

.. _kafka-caveat:

Важное замечание
----------------

В данном документе предполагается, что читатель знает, как настроить кластер Kafka и координатор Zookeeper, а также
обеспечить их безопасность при обычном использовании путем предотвращения несанкционированного доступа. Единственная
цель данного руководства - определить шаги, необходимые для того, чтобы множество упорядочивающих узлов Hyperledger
Fabric использовало кластер Kafka и обеспечивало работу службы упорядочения в блокчейн-сети.

Для получения информации о том, какую роль в сети играют упорядочивающие узлы при выполнении транзакций, обратитесь к
статье :doc:`orderer/ordering_service`.

Процесс установки упорядочивающего узла описан в статье :doc:`orderer_deploy`.

О настройке службы упорядочения на основе Raft можно прочитать в статье :doc:`raft_configuration`.

Общая картина
-------------

Каждый канал в сети соответствует отдельному топику в Kafka, состоящему из одного раздела. Когда упорядочивающий узел
получает транзакции через вызов удаленной процедуры ``Broadcast``, он проверяет, есть ли у вызвавшего эту процедуру
клиента разрешение на запись в канал, а затем передает (в терминах Kafka - производит) эти транзакции в соответствующий
раздел в Kafka. Этот раздел также используется упорядочивающими узлами, которые локально группируют транзакции в блоки.
Блоки сохраняются в локальном реестре этих узлов, а затем передаются клиентам через вызов удаленной процедуры
``Deliver``. Если вас интересуют более низкоуровневые подробности данного процесса, обратитесь к
`документу с его описанием <https://docs.google.com/document/d/19JihmW-8blTzN99lAubOfseLUZqdrB6sBR0HsRgCAnY/edit>`_.
**Рисунок 8** схематически показывает описанный выше процесс.

Шаги
-----

Пусть ``K`` и ``Z`` - количество узлов в кластере Kafka и координаторе Zookeeper соответственно. Тогда:

1. Минимальное значение ``K`` должно быть равно 4 (как будет показано на шаге 6, это минимальное количество узлов,
   необходимое для обеспечения отказоустойчивости; т.е. если один из четырех брокеров выйдет из строя, все каналы
   по-прежнему будут доступны для чтения и записи, а также будет возможно создавать новые каналы).

2. ``Z`` должно быть равно 3, 5 или 7. Это должно быть нечетное число, чтобы избежать сценариев с двумя ведущими узлами
   (лидерами), и оно должно быть больше 1, чтобы не было единой точки отказа. Количество узлов Zookeeper больше 7
   считается излишним.

Далее выполните следующие действия:

3. Для упорядочивающих узлов: **Закодируйте информацию о Kafka в начальном блоке сети**. Если вы используете
   ``configtxgen``, измените файл ``configtx.yaml``. Или выберете предустановленный профиль для начального блока
   системного канала так, чтобы:

   * ``Orderer.OrdererType`` был установлен в ``kafka``.
   * ``Orderer.Kafka.Brokers`` содержал адреса *минимум двух* брокеров Kafka в вашем кластере в виде ``IP:port``.
     Список не обязательно должен быть полным (это ваши загрузочные брокеры).

4. Для упорядочивающих узлов: **Установите максимальный размер блока.** Размер каждого блока в байтах (без учета
   заголовков) не будет превышать значения ``Orderer.AbsoluteMaxBytes``, установленного в файле ``configtx.yaml``. Пусть
   выбранное значение будет ``A``. Имейте в виду, что оно повлияет на настройку брокеров Kafka на шаге 6.

5. Для упорядочивающих узлов: **Создайте начальный блок.** Используйте ``configtxgen``. Настройки, сделанные на шагах
   3 и 4, являются общесистемными и будут применяться по всей сети для всех упорядочивающих узлов. Запомните
   расположение файла начального блока.

6. Для кластера Kafka: **Произведите необходимую настройку брокеров Kafka.** Убедитесь, что для каждого брокера Kafka
   установлены следующие ключи:

   * ``unclean.leader.election.enable = false``. Согласованность данных является ключевым фактором в блокчейн-среде.
     Нельзя допустить, чтобы лидер канала был выбран вне синхронизированного набора реплик, иначе мы рискуем
     потерять результаты, полученные предыдущим лидером, и в результате переписать цепочку блоков, которую создают
     упорядочивающие узлы.

   * ``min.insync.replicas = M``. Выберете ``M`` таким, чтобы было верным неравенство ``1 < M < N`` (смотрите
     описание ключа ``default.replication.factor`` ниже). Данные считаются записанными, если они записаны по крайней
     мере в ``M`` реплик (которые в этом случае считаются синхронными и принадлежат множеству синхронных реплик,
     или ISR). В любом другом случае операция записи возвращает ошибку. Тогда:

     * Если до ``N - M`` реплик из ``N``, в которые записываются данные канала, становятся недоступными, операции
       продолжаются нормально.

     * Если большее количество реплик становится недоступным, Kafka не может поддерживать множество синхронных реплик и
       прекращает принимать записи. При этом чтение продолжает работать. Канал снова становится доступным для записи,
       когда ``M`` реплик становятся синхронными.

   * ``default.replication.factor = N``. Выберете ``N`` таким, чтобы было верным неравенство ``N < K``. Коэффициент
     репликации ``N`` означает, что данные каждого канала будут реплицироваться на ``N`` брокеров. Это кандидаты в
     множество синхронных реплик канала. Как было отмечено выше в разделе о ``min.insync.replicas section``, не все
     брокеры должны быть доступны постоянно. ``N`` следует устанавливать *строго меньше* ``K``, потому что создание
     канала не может быть выполнено, если доступно менее ``N`` брокеров. Таким образом, если установить ``N=K``, то
     выход из строя одного брокера приведет к тому, что в блокчейн-сети не смогут быть созданы новые каналы, а значит
     отказоустойчивость службы упорядочения будет отсутствовать.

     Из сказанного выше следует, что минимальные значения для ``M`` и ``N`` равны 2 и 3 соответственно. Такая
     конфигурация позволяет создавать новые каналы, а существующим каналам быть доступными для записи.

   * ``message.max.bytes`` и ``replica.fetch.max.bytes`` должны иметь значение большее, чем ``A``, которое было
     установлено для параметра ``Orderer.AbsoluteMaxBytes`` на шаге 4. Оставьте небольшой запась для заголовков: 1 Мб
     более чем достаточно. Применимо следующее условие:

     ::

         Orderer.AbsoluteMaxBytes < replica.fetch.max.bytes <= message.max.bytes

     (Для полноты картины отметим, что значение ``message.max.bytes`` должно быть строго меньше, чем значение
     ``socket.request.max.bytes``, которое по умолчанию равно 100 МБ. Если вам необходимо создавать блоки размером
     больше 100 МБ, придется изменить значение ``brokerConfig.Producer.MaxMessageBytes`` в исходном коде в файле
     ``fabric/orderer/kafka/config.go`` и пересобрать бинарные файлы. Делать это не рекомендуется.)

   * ``log.retention.ms = -1``. Пока в службе упорядочения нет поддержки обрезки журналов Kafka, следует отключить
     сохранение на основе времени и предотвратить истечение срока действия сегментов разделов Kafka (сохранение на
     основе размера - см. ``log.retention.bytes`` — отключено по умолчанию на момент написания этой статьи, поэтому нет
     необходимости устанавливать его явно).

7. Для упорядочивающих узлов: **Укажите расположение начального блока каждому упорядочивающему узлу.** Измените
   параметр ``General.BootstrapFile`` в файле ``orderer.yaml``, чтобы он указывал на начальный блок, созданный на
   шаге 5. В процессе установки этого параметра убедитесь, что остальные ключи в этом YAML-файле установлены верно.

8. Для упорядочивающих узлов: **Настройка интервалов опроса и времени ожидания.** (Необязательный шаг)

   * В секции ``Kafka.Retry`` в файле ``orderer.yaml`` можно настроить частоту запросов метаданных/поставщиков/потребителей,
     а также время ожидания сокетов (все эти параметры ожидаемы у поставщиков или потребителей данных Kafka).

   * Кроме того, когда создается новый канал или перезагружается существующий (в случае перезапуска службы
     упорядочения), служба упорядочения взаимодействует с кластером Kafka следующим способом:

     * Создает поставщик Kafka (писателя) для раздела Kafka, соответствующего каналу. Далее использует этот
       поставщик для отправки сообщения ``CONNECT`` в этот раздел. Далее создается потребитель (читатель) Kafka для
       этого раздела.

     * Можно настроить частоту повторения для этих действий на случай, если одно их них не удается. В частности, они
       будут повторяться через интервал, определенный в ``Kafka.Retry.ShortInterval``, в течение ``Kafka.Retry.ShortTotal``,
       а затем через ``Kafka.Retry.LongInterval`` в течение ``Kafka.Retry.LongTotal`` до тех пор, пока не выполнятся
       успешно. Обратите внимание, что служба упорядочения не сможет писать в канал или читать из него до тех пор, пока
       все вышеперечисленные действия не будут успешно завершены.

9. **Настройте упорядочивающие узлы и кластер Kafka так, чтобы они обменивались данными по протоколу SSL.**
   (Шаг не обязательный, но крайне рекомендуемый.) Обратитесь к `руководству Confluent <https://docs.confluent.io/2.0.0/kafka/ssl.html>`_
   для настройки части, относящейся к кластеру Kafka, и установите ключи в разделе ``Kafka.TLS`` в файле ``orderer.yaml``
   на каждом упорядочивающем узле сооветствующим образом.

10. **Запустите узлы в следующем порядке: координатор Zookeeper, кластер Kafka, узлы службы упорядочения**

Дополнительные замечания
------------------------

1. **Предпочтительный размер сообщения.** На шаге 4 можно установить предпочтительный размер блоков, установив значение
   ключа ``Orderer.Batchsize.PreferredMaxBytes``. Kafka обеспечивает более высокую пропускную способность при работе с
   относительно небольшими сообщениями; ориентируйтесь на значение не больше 1 МБ.

2. **Использование переменных окружения для переопределения настроек.** При использовании Docker-образов Kafka и
   Zookeeper, поставляемых вместе Fabric (см. ``images/kafka`` и ``images/zookeeper`` соответственно), у вас есть
   возможность переопределить настройки брокера Kafka или сервера Zookeeper с помощью переменных окружения. Для этого
   замените точки в ключе конфигурации на символы подчеркивания. Например, ``KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false``
   позволит переопределить значение по умолчанию для ключа ``unclean.leader.election.enable``. То же самое относится
   и к *локальной* конфигурации узлов службы упорядочения, то есть к тому, что можно определить через файл ``orderer.yaml``.
   Например, ``ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s`` позволяет переопределить значение по умолчанию для ключа
   ``Orderer.Kafka.Retry.ShortInterval``.

Совместимость версий протокола Kafka
------------------------------------

Fabric использует `клиентскую библиотеку sarama <https://github.com/Shopify/sarama>`_ и поставляет ее версию, которая
поддерживает версии Kafka от 0.10 до 1.0, но при этом известно, что она работает и с более старыми версиями.

Используя ключ ``Kafka.Version`` в файле ``orderer.yaml``, вы можете указать, какая версия протокола Kafka используется
для связи с брокерами кластера Kafka. Брокеры Kafka обратно совместимы с более старыми версиями протокола. Из-за
обратной совместимости брокеров Kafka со старыми версиями протокола их обновление до более новой версии не требует
обновления значения ключа ``Kafka.Version``, но при использовании более старой версии протокола производительность
кластера Kafka `может снизиться <https://kafka.apache.org/documentation/#upgrade_11_message_format>`_.

Отладка
-------

Установите переменной окружения ``FABRIC_LOGGING_SPEC`` значение ``DEBUG``, а ключу ``Kafka.Verbose`` в файле
``orderer.yaml`` - значение ``true``.

.. Licensed under Creative Commons Attribution 4.0 International License
https://creativecommons.org/licenses/by/4.0/
